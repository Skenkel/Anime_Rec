{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose:\n",
    "This notebook contains the code that is use to generate weights for a \"Status predictor\" classifier neural net. \n",
    "This treats the status of \"Completed\" as a target, and all other status \"Dropped, On Hold, Watcing, Backlog\" as a 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras)\n"
     ]
    }
   ],
   "source": [
    "#!pip install keras # Because this has been run on several different docker images, need to make sure keras is installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  7 18:12:55 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 384.69                 Driver Version: 384.69                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\r\n",
      "| 20%   37C    P8    18W / 250W |  10691MiB / 11170MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # Need to check to confirm that the GPU memory is not in use from another process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Embedding, merge\n",
    "import keras.layers\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#import pydotplus as pydot \n",
    "#import graphviz\n",
    "#from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_score_train = pd.read_csv('mal_scores_train_nonzero_v2.csv')# Time to load in all of the data. \n",
    "ratings_score_test = pd.read_csv('mal_scores_test_nonzero_v2.csv')# the v2 version of this data has no \"test\" users who aren't present in the training data. \n",
    "ratings_no_score_train = pd.read_csv('mal_scores_train_zero.csv') \n",
    "ratings_no_score_test = pd.read_csv('mal_scores_test_zero.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10914898, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_score_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1210873, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_score_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071354, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_no_score_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9632967, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_no_score_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_score_train.head() #Check to make sure the data is good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = ratings_score_train[['animeid','userid','status']]\n",
    "noscore = ratings_no_score_train[['animeid','userid','status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concatlist = [scores,noscore]\n",
    "data_train = pd.concat(concatlist,ignore_index=True) # I only care about anime, user and status, so I need to take those from the two sources and merge them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_scores = ratings_score_test[['animeid','userid','status']]\n",
    "test_noscores =ratings_no_score_test[['animeid','userid','status']]\n",
    "concatlist2 = [test_scores,test_noscores]\n",
    "data_test = pd.concat(concatlist2,ignore_index=True) # Creating the test set to judge the classifier against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train['target']=data_train['status']==\"COMPLETED\"\n",
    "data_train['target']=data_train['target'].astype(int) # turning completed in 1s everyhing else is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['target']=data_test['status']==\"COMPLETED\"\n",
    "data_test['target']=data_test['target'].astype(int) # making the target variable for the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to re-order and dataprep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#userid mapping\n",
    "users1 = ratings_score_train.userid.unique()\n",
    "users2 = ratings_score_test.userid.unique()\n",
    "users3 = ratings_no_score_train.userid.unique()\n",
    "#animeid mapping\n",
    "anime1= ratings_score_train.animeid.unique()\n",
    "anime2=ratings_score_test.animeid.unique()\n",
    "anime3=ratings_no_score_train.animeid.unique()\n",
    "#statusid mapping\n",
    "status = ratings_score_train.status.unique() #only need to do this on one of the dataframes because there are only 6 status, and a status cannot be missing. \n",
    "n_status = ratings_score_train.status.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = set(users1).union(set(users2)).union(set(users3))\n",
    "animes = set(anime1).union(set(anime2)).union(set(anime3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userid2idx=np.load(\"user.npy\").item()\n",
    "animeid2idx=np.load(\"anime.npy\").item()\n",
    "#From prior parts of the model, the user and anime embedding id is stored in this dictionary, to make sure that anime have the same embedding when they are loaded into the recommendation engine.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = len(userid2idx)\n",
    "n_animes = len(animeid2idx) # setting up the variables for the model later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['anime_id_emb'] = data_test.animeid.apply(lambda x: animeid2idx[x])\n",
    "data_train['anime_id_emb'] = data_train.animeid.apply(lambda x: animeid2idx[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['user_id_emb'] = data_test.userid.apply(lambda x: userid2idx[x])\n",
    "data_train['user_id_emb'] = data_train.userid.apply(lambda x: userid2idx[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_factors = 36 #The number of hidden factors each user and anime is transformed into.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()# making sure we are ready to model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the baseline to know what the model's baseline accuracy is. \n",
    "data_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_input_anime1(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg),name='Embed_Anime_Hidden_Factors')(inp)\n",
    "def embedding_input_user1(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg),name='Embed_User_Hidden_Factors')(inp)\n",
    "\n",
    "user_in1, u1 = embedding_input_user1('user_id_in', n_users+15, n_factors, 1e-6) # the extra 15 is headroom for unused embeddings in the recommendation stae. \n",
    "anime_in1, a1 = embedding_input_anime1('anime_id_in', n_animes, n_factors, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn1_ for status prediction\n",
    "x = merge([u1, a1], mode='concat', name='All_Factors_on_one_layer')\n",
    "x = Flatten()(x)\n",
    "#x = Dropout(0.55, name='Prevent_overfit')(x) # this layer made the model worse. \n",
    "x = Dense(70, activation='relu',name='Random_HF_Interactions')(x)\n",
    "x = Dropout(0.35,name='Prevent_overfit2')(x)\n",
    "x = Dense(16, activation='relu',name='Random_HF_Interactions2')(x)\n",
    "x = Dropout(0.1, name='Prevent_overfit')(x)\n",
    "x = Dense(1,name='Final_Interactions',activation='sigmoid')(x)\n",
    "\n",
    "nn1 = Model([user_in1, anime_in1], x)\n",
    "nn1.compile(Adam(0.001), loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's fit the model.\n",
    "nn1.fit([data_train.user_id_emb, data_train.anime_id_emb], data_train.target, batch_size=5120, epochs=5, \n",
    "          validation_data=([data_test.user_id_emb, data_test.anime_id_emb], data_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " nn1.lr=.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1.fit([data_train.user_id_emb, data_train.anime_id_emb], data_train.target, batch_size=5120, epochs=9, \n",
    "          validation_data=([data_test.user_id_emb, data_test.anime_id_emb], data_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1.fit([data_train.user_id_emb, data_train.anime_id_emb], data_train.target, batch_size=10120, epochs=3, \n",
    "          validation_data=([data_test.user_id_emb, data_test.anime_id_emb], data_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn1.save_weights('nn_score_weights_pred.h5') # and with this save, the model is complete, and these weights are ready for the recommender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
